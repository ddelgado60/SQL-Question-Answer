{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Generative QA\n",
    "\n",
    "This workflow will finetune T5 Seq-to-Seq model on a SQL dataset that contains natural language questions, SQL context, and SQL results that correctly answer the question. The workflow provides the high level overview of how the system works. An accompanying application will allow a user to have this abstracted away with a pretrained model to interact with.\n",
    "\n",
    "Main components:\n",
    "\n",
    "Question: natural language questions\\\n",
    "Context: Building of table(s), relevant columns, and synethetic data\\\n",
    "Answer: Correct SQL query that answers the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu device\n",
      "GPU: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "if device == 'gpu':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sql_data = load_dataset('gretelai/synthetic_text_to_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We only require the sql_prompt, sql_context, and sql columns in this dataset. These will be extracted and tokenized using F5's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def tokenize(inputs: Dataset) -> Dataset:\n",
    "    '''\n",
    "    Tokenizes the SQL prompt using the T5 tokenizer. \n",
    "    Question and context will be appended as inputs. The SQL query will be the label.\n",
    "\n",
    "    Args:\n",
    "        inputs (datasets.Dataset): The dataset to tokenize (train or test)\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: The tokenized dataset\n",
    "    '''\n",
    "    concat = [f\"Translate to SQL: {q} Context: {c}\" for q, c in zip(inputs[\"sql_prompt\"], inputs[\"sql_context\"])]\n",
    "    tokenized_inputs = tokenizer(concat, max_length=500, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    tokenized_labels = tokenizer(inputs['sql'], max_length=500, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_inputs['attention_mask'],\n",
    "        'labels': tokenized_labels['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = sql_data['train'].map(tokenize, batched=True, remove_columns=sql_data['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = sql_data['test'].map(tokenize, batched=True, remove_columns=sql_data['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune F5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"sql_generator_f5\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tests(question, context, model):\n",
    "    '''\n",
    "    Formats the question and context for the model to generate a SQL query\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to ask the model\n",
    "        context (str): The context to provide the model\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted question and context\n",
    "    '''\n",
    "    input_text = f\"Translate to SQL: {question} Context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_length=512, do_sample=True, temperature=0.6, top_k=50, top_p=0.95)\n",
    "    generated_sql = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import sqlparse\n",
    "\n",
    "model_path = \"sql_generator_f5/checkpoint-75000\"  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT name\n",
      "FROM employees\n",
      "WHERE id = 1;\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the name of the employee with id 1?\"\n",
    "context = \"\"\"\n",
    "CREATE TABLE employees (id INT, name TEXT);\n",
    "INSERT INTO employees (id, name) VALUES \n",
    "(1, 'Alice'), \n",
    "(2, 'Bob');\n",
    "\"\"\"\n",
    "\n",
    "generated_sql = format_tests(question, context, model)\n",
    "sql_formatted = sqlparse.format(generated_sql, reindent=True, keyword_case='upper')\n",
    "print(sql_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT department_name,\n",
      "       COUNT(*) AS total_employees\n",
      "FROM employees\n",
      "GROUP BY department_id;\n"
     ]
    }
   ],
   "source": [
    "question = \"How many employees work in each department?\"\n",
    "context = \"\"\"\n",
    "CREATE TABLE employees (id INT, name TEXT, department_id INT);\n",
    "INSERT INTO employees (id, name, department_id) VALUES \n",
    "(1, 'Alice', 1), \n",
    "(2, 'Bob', 1), \n",
    "(3, 'Charlie', 2), \n",
    "(4, 'David', 2), \n",
    "(5, 'Eve', 3);\n",
    "\n",
    "CREATE TABLE departments (department_id INT, department_name TEXT);\n",
    "INSERT INTO departments (department_id, department_name) VALUES \n",
    "(1, 'HR'), \n",
    "(2, 'Engineering'), \n",
    "(3, 'Marketing');\n",
    "\"\"\"\n",
    "\n",
    "generated_sql = format_tests(question, context, model)\n",
    "sql_formatted = sqlparse.format(generated_sql, reindent=True, keyword_case='upper')\n",
    "print(sql_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT p.product_name,\n",
      "       SUM(s.quantity) AS total_quantity\n",
      "FROM sales s\n",
      "JOIN products p ON s.product_id = p.product_id\n",
      "WHERE s.sale_date BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY p.product_name;\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the total quantity of each product sold in 2023?\"\n",
    "context = \"\"\"\n",
    "CREATE TABLE products (product_id INT, product_name TEXT);\n",
    "INSERT INTO products (product_id, product_name) VALUES \n",
    "(1, 'Laptop'), \n",
    "(2, 'Phone'), \n",
    "(3, 'Tablet');\n",
    "\n",
    "CREATE TABLE sales (sale_id INT, product_id INT, quantity INT, sale_date DATE);\n",
    "INSERT INTO sales (sale_id, product_id, quantity, sale_date) VALUES \n",
    "(1, 1, 5, '2023-01-10'), \n",
    "(2, 2, 10, '2023-02-15'), \n",
    "(3, 2, 7, '2023-03-20'), \n",
    "(4, 3, 3, '2023-04-25'), \n",
    "(5, 1, 2, '2023-08-30'), \n",
    "(6, 3, 8, '2023-12-10');\n",
    "\"\"\"\n",
    "\n",
    "generated_sql = format_tests(question, context, model)\n",
    "sql_formatted = sqlparse.format(generated_sql, reindent=True, keyword_case='upper')\n",
    "print(sql_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Files\n",
    "\n",
    "Model may be found on HuggingFace at https://huggingface.co/DevD60/sql_generator_f5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql-qa-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
